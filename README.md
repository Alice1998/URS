## A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models

## Data

- URS_dataset
  - real-world cases reported in the user survey
- Benchmark_results
  - evaluation process data and final results of 10 tested LLMs
- human_pair_comparison
  - annotators pairwise comparison of LLMs

## Code

```
(fill-in api keys)
python getServiceOutput.py --model GPT-4o --field all
python evaluation.py --model GPT-4o --field all
```

